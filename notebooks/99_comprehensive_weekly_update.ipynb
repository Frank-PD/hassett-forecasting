{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comprehensive Weekly Forecast Update - FULL RUN (All 18 Models)\n\n**Purpose**: Complete weekly update with ALL forecasting models\n\n## What This Notebook Does:\n1. **Fetch Actuals** - Query last week's actual shipments from Databricks\n2. **Evaluate ALL 18 Models** - Run comprehensive model comparison\n3. **Update Routing** - Determine winning models and update routing table\n4. **Track Performance** - Store results in performance tracking database\n5. **Generate Forecast** - Create current week's forecast using optimal models\n6. **Export Files** - Save all CSVs and summaries\n\n## Models Included (18 Total):\n\n**Traditional Models (13):**\n- 01_Historical_Baseline, 02_Recent_2W, 03_Recent_4W_HYBRID, 04_Recent_8W\n- 05_Trend_Adjusted, 06_Prior_Week, 07_Same_Week_Last_Year, 08_Week_Specific\n- 09_Exp_Smoothing, 10_Probabilistic, 11_Hybrid_Week_Blend\n- 12_Median_Recent, 13_Weighted_Recent_Week\n\n**Advanced Models (5):**\n- 14_SARIMA (Time series), 15_ML_Classifier, 16_ML_Regressor\n- 17_Lane_Adaptive, 18_Clustering\n\n## Runtime: 30-60 minutes\n\n‚è±Ô∏è SARIMA model fitting is the bottleneck (~30-40 min for 1,500 routes)\n\n## How It Works:\n\n**Weeks are automatically calculated:**\n- **Evaluation Week**: Last week (current week - 1) - has actuals\n- **Forecast Week**: Current week - what we're forecasting for\n\nExample (running on Dec 15, 2025):\n- Current week: 51\n- Evaluation week: 50 (last week, has actuals)\n- Forecast week: 51 (this week)\n\n**To override**, manually set `EVALUATION_WEEK` and `FORECAST_WEEK` in the configuration cell.\n\nRun all cells to update everything!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nimport sys\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom databricks import sql\nimport sqlite3\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Progress bars\nfrom tqdm.auto import tqdm\n\n# Add src to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root / 'src'))\n\n# Import model functions\nfrom forecast_comprehensive_all_models import ComprehensiveModels\n\nprint(\"‚úÖ Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== CONFIGURATION ==========\n# Automatically calculate weeks based on current date\n\ntoday = datetime.now()\ncurrent_week = today.isocalendar()[1]  # Get ISO week number\ncurrent_year = today.year\n\n# Evaluation week = last week (week that just ended, has actuals)\nEVALUATION_WEEK = current_week - 1\n\n# Forecast week = current week (the week we're in now)\nFORECAST_WEEK = current_week\n\n# Handle year boundary (if we're in week 1, evaluation week is last week of previous year)\nif EVALUATION_WEEK < 1:\n    EVALUATION_WEEK = 52  # Last week of previous year\n    EVALUATION_YEAR = current_year - 1\n    FORECAST_YEAR = current_year\nelse:\n    EVALUATION_YEAR = current_year\n    FORECAST_YEAR = current_year\n\n# Databricks connection\nDATABRICKS_CONFIG = {\n    \"server_hostname\": \"adb-434028626745069.9.azuredatabricks.net\",\n    \"http_path\": \"/sql/1.0/warehouses/23a9897d305fb7e2\",\n    \"auth_type\": \"databricks-oauth\"\n}\n\nTABLE_NAME = \"decus_domesticops_prod.dbo.tmp_hassett_report\"\n\n# Output timestamp\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n\nprint(f\"üìã Auto-Configuration (Today: {today.strftime('%Y-%m-%d')}):\")\nprint(f\"  Current Week: {current_week}\")\nprint(f\"  Evaluation Week: {EVALUATION_WEEK}, {EVALUATION_YEAR} (last week - has actuals)\")\nprint(f\"  Forecast Week: {FORECAST_WEEK}, {FORECAST_YEAR} (current week)\")\nprint(f\"  Timestamp: {TIMESTAMP}\")\nprint(f\"\\nüí° Note: Weeks are auto-calculated. To override, manually set EVALUATION_WEEK and FORECAST_WEEK.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect to Databricks and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Connect to Databricks\nconn = sql.connect(**DATABRICKS_CONFIG)\nprint(\"‚úÖ Connected to Databricks\")\n\n# Load historical data (4 years lookback)\ndef load_historical_data(conn, target_week, target_year, table_name, years=4):\n    year_start = datetime(target_year, 1, 1)\n    target_date = year_start + timedelta(weeks=target_week - 1)\n    lookback_date = target_date - timedelta(days=365 * years)\n\n    query = f\"\"\"\n    SELECT\n        DATE_SHIP as date,\n        ODC, DDC, ProductType,\n        PIECES as pieces,\n        weekofyear(DATE_SHIP) as week,\n        YEAR(DATE_SHIP) as year,\n        dayofweek(DATE_SHIP) as dayofweek\n    FROM {table_name}\n    WHERE DATE_SHIP >= '{lookback_date.strftime('%Y-%m-%d')}'\n        AND DATE_SHIP < '{target_date.strftime('%Y-%m-%d')}'\n        AND ProductType IN ('MAX', 'EXP')\n        AND ODC IS NOT NULL\n        AND DDC IS NOT NULL\n    ORDER BY DATE_SHIP DESC\n    \"\"\"\n\n    print(f\"üìä Loading {years} years of historical data (up to week {target_week}, {target_year})...\")\n    cursor = conn.cursor()\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    columns = [desc[0] for desc in cursor.description]\n    df = pd.DataFrame(rows, columns=columns)\n    df['date'] = pd.to_datetime(df['date'])\n    print(f\"‚úÖ Loaded {len(df):,} historical records\")\n    return df\n\n# Load data up to evaluation week\ndf_historical = load_historical_data(conn, EVALUATION_WEEK, EVALUATION_YEAR, TABLE_NAME, years=4)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Actuals for Evaluation Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Query actuals for the evaluation week\ndef get_week_actuals(conn, week, year, table_name):\n    query = f\"\"\"\n    SELECT \n        ODC,\n        DDC,\n        ProductType,\n        dayofweek(DATE_SHIP) as dayofweek,\n        SUM(PIECES) as actual_pieces\n    FROM {table_name}\n    WHERE weekofyear(DATE_SHIP) = {week}\n        AND YEAR(DATE_SHIP) = {year}\n        AND ProductType IN ('MAX', 'EXP')\n    GROUP BY ODC, DDC, ProductType, dayofweek(DATE_SHIP)\n    ORDER BY ODC, DDC, ProductType, dayofweek\n    \"\"\"\n    \n    print(f\"üìä Querying actuals for week {week}, {year}...\")\n    cursor = conn.cursor()\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    columns = [desc[0] for desc in cursor.description]\n    df = pd.DataFrame(rows, columns=columns)\n    print(f\"‚úÖ Found {len(df):,} route-day actuals\")\n    print(f\"   Total pieces: {df['actual_pieces'].sum():,.0f}\")\n    return df\n\ndf_actuals = get_week_actuals(conn, EVALUATION_WEEK, EVALUATION_YEAR, TABLE_NAME)\n\n# Show sample\nprint(\"\\nSample actuals:\")\ndisplay(df_actuals.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate All Model Forecasts for Evaluation Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get all unique routes from actuals\nroutes = df_actuals[['ODC', 'DDC', 'ProductType', 'dayofweek']].drop_duplicates()\nprint(f\"üìä Generating forecasts for {len(routes):,} route-days using ALL 18 MODELS...\\n\")\nprint(f\"‚è±Ô∏è  Estimated time: 30-60 minutes (SARIMA is slow)\\n\")\n\n# ALL 18 MODELS - Full comprehensive analysis\nmodel_functions = [\n    # Traditional models (13)\n    ('01_Historical_Baseline', ComprehensiveModels.model_01_historical_baseline),\n    ('02_Recent_2W', ComprehensiveModels.model_02_recent_2w_avg),\n    ('03_Recent_4W_HYBRID', ComprehensiveModels.model_03_recent_4w_avg),\n    ('04_Recent_8W', ComprehensiveModels.model_04_recent_8w_avg),\n    ('05_Trend_Adjusted', ComprehensiveModels.model_05_trend_adjusted),\n    ('06_Prior_Week', ComprehensiveModels.model_06_prior_week),\n    ('07_Same_Week_Last_Year', ComprehensiveModels.model_07_same_week_last_year),\n    ('08_Week_Specific', ComprehensiveModels.model_08_week_specific_historical),\n    ('09_Exp_Smoothing', ComprehensiveModels.model_09_exponential_smoothing),\n    ('10_Probabilistic', ComprehensiveModels.model_10_probabilistic),\n    ('11_Hybrid_Week_Blend', ComprehensiveModels.model_11_hybrid_week_blend),\n    ('12_Median_Recent', ComprehensiveModels.model_12_median_recent),\n    ('13_Weighted_Recent_Week', ComprehensiveModels.model_13_weighted_recent_week),\n    # Advanced models (5)\n    ('14_SARIMA', ComprehensiveModels.model_14_sarima),\n    ('15_ML_Classifier_Simple_Vol', ComprehensiveModels.model_15_ml_classifier_simple_volume),\n    ('16_ML_Regressor', ComprehensiveModels.model_16_ml_regressor),\n    ('17_Lane_Adaptive', ComprehensiveModels.model_17_lane_adaptive),\n    ('18_Clustering', ComprehensiveModels.model_18_clustering),\n]\n\n# Generate forecasts for each route\nresults = []\n\n# Progress bar\npbar = tqdm(routes.iterrows(), total=len(routes), desc=\"Evaluating all 18 models\")\nfor idx, route in pbar:\n    odc, ddc, product, dow = route['ODC'], route['DDC'], route['ProductType'], route['dayofweek']\n    \n    # Update progress bar with current route\n    pbar.set_postfix({'Route': f\"{odc}-{ddc}-{product}\"})\n    \n    # Get route history\n    route_data = df_historical[\n        (df_historical['ODC'] == odc) &\n        (df_historical['DDC'] == ddc) &\n        (df_historical['ProductType'] == product) &\n        (df_historical['dayofweek'] == dow)\n    ].sort_values('date', ascending=False)\n    \n    # Get actual\n    actual = df_actuals[\n        (df_actuals['ODC'] == odc) &\n        (df_actuals['DDC'] == ddc) &\n        (df_actuals['ProductType'] == product) &\n        (df_actuals['dayofweek'] == dow)\n    ]['actual_pieces'].values[0] if len(df_actuals[\n        (df_actuals['ODC'] == odc) &\n        (df_actuals['DDC'] == ddc) &\n        (df_actuals['ProductType'] == product) &\n        (df_actuals['dayofweek'] == dow)\n    ]) > 0 else 0\n    \n    result = {\n        'route_key': f\"{odc}|{ddc}|{product}|{dow}\",\n        'ODC': odc,\n        'DDC': ddc,\n        'ProductType': product,\n        'dayofweek': dow,\n        'Actual': actual\n    }\n    \n    # Run each model\n    for model_name, model_func in model_functions:\n        try:\n            forecast = model_func(route_data, EVALUATION_WEEK, EVALUATION_YEAR, product)\n            result[model_name] = max(0, forecast)\n        except Exception as e:\n            # If model fails, use 0 (common for SARIMA with insufficient data)\n            result[model_name] = 0\n    \n    results.append(result)\n\ndf_forecasts = pd.DataFrame(results)\nprint(f\"\\n‚úÖ Generated forecasts for {len(df_forecasts):,} routes using all 18 models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate Errors and Find Winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate errors for each model\nmodel_cols = [col for col in df_forecasts.columns if col not in ['route_key', 'ODC', 'DDC', 'ProductType', 'dayofweek', 'Actual']]\n\nprint(f\"üìä Calculating errors for {len(model_cols)} models...\")\nprint(f\"   Models evaluated: {', '.join(model_cols)}\\n\")\n\nfor col in model_cols:\n    error_col = f\"{col}_Error%\"\n    df_forecasts[error_col] = np.where(\n        df_forecasts['Actual'] > 0,\n        abs((df_forecasts[col] - df_forecasts['Actual']) / df_forecasts['Actual'] * 100),\n        np.where(df_forecasts[col] > 0, 999, 0)\n    )\n\n# Find winner for each route\nerror_cols = [col for col in df_forecasts.columns if col.endswith('_Error%')]\ndf_forecasts['Winner_Model'] = df_forecasts[error_cols].abs().idxmin(axis=1).str.replace('_Error%', '')\ndf_forecasts['Winner_Error%'] = df_forecasts[error_cols].abs().min(axis=1)\n\nprint(f\"‚úÖ Winners determined!\")\nprint(f\"\\nüèÜ Model Win Summary (Top 10):\")\nwin_summary = df_forecasts['Winner_Model'].value_counts()\nfor i, (model, count) in enumerate(win_summary.head(10).items(), 1):\n    pct = count / len(df_forecasts) * 100\n    print(f\"  {i:2}. {model:<30} {count:4,} routes ({pct:5.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Comprehensive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "output_file = project_root / f'comprehensive_all_models_week{EVALUATION_WEEK}.csv'\n",
    "df_forecasts.to_csv(output_file, index=False)\n",
    "print(f\"üíæ Saved: {output_file.name}\")\n",
    "print(f\"   Size: {len(df_forecasts):,} routes x {len(df_forecasts.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Update Routing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create routing table with winners\n",
    "routing_table = df_forecasts[[\n",
    "    'route_key', 'ODC', 'DDC', 'ProductType', 'dayofweek',\n",
    "    'Winner_Model', 'Winner_Error%', 'Actual'\n",
    "]].copy()\n",
    "\n",
    "routing_table.columns = [\n",
    "    'route_key', 'ODC', 'DDC', 'ProductType', 'dayofweek',\n",
    "    'best_model', 'best_error', 'actual'\n",
    "]\n",
    "\n",
    "# Add confidence levels\n",
    "def assign_confidence(error):\n",
    "    if error == 999:\n",
    "        return 'LOW'\n",
    "    elif error <= 20:\n",
    "        return 'HIGH'\n",
    "    elif error <= 50:\n",
    "        return 'MEDIUM'\n",
    "    else:\n",
    "        return 'LOW'\n",
    "\n",
    "routing_table['confidence'] = routing_table['best_error'].apply(assign_confidence)\n",
    "\n",
    "# Save routing table\n",
    "routing_file = project_root / f'route_model_routing_{TIMESTAMP}.csv'\n",
    "routing_table.to_csv(routing_file, index=False)\n",
    "print(f\"üíæ Saved: {routing_file.name}\")\n",
    "\n",
    "# Also save as current routing table\n",
    "current_routing_file = project_root / 'route_model_routing_table.csv'\n",
    "routing_table.to_csv(current_routing_file, index=False)\n",
    "print(f\"üíæ Updated: {current_routing_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Update Performance Tracking Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save performance summary\nperformance_summary = {\n    'timestamp': TIMESTAMP,\n    'evaluation_week': EVALUATION_WEEK,\n    'evaluation_year': EVALUATION_YEAR,\n    'total_routes': len(df_forecasts),\n    'total_actuals': int(df_forecasts['Actual'].sum()),\n    'model_wins': win_summary.to_dict()\n}\n\nperf_file = project_root / f'model_performance_summary_{TIMESTAMP}.json'\nwith open(perf_file, 'w') as f:\n    json.dump(performance_summary, f, indent=2)\n\nprint(f\"üíæ Saved: {perf_file.name}\")\nprint(f\"\\nüìä Performance Summary:\")\nprint(json.dumps(performance_summary, indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Forecast for Next Week Using Winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load updated historical data for forecast week\ndf_historical_forecast = load_historical_data(conn, FORECAST_WEEK, FORECAST_YEAR, TABLE_NAME, years=4)\n\nprint(f\"\\nüìä Generating forecasts for week {FORECAST_WEEK} using optimal models...\\n\")\n\n# Get routes to forecast (use routing table)\nforecast_results = []\n\n# Progress bar\nfor idx, route in tqdm(routing_table.iterrows(), total=len(routing_table), desc=\"Generating forecasts\"):\n    odc, ddc, product, dow = route['ODC'], route['DDC'], route['ProductType'], route['dayofweek']\n    best_model = route['best_model']\n    \n    # Get route history\n    route_data = df_historical_forecast[\n        (df_historical_forecast['ODC'] == odc) &\n        (df_historical_forecast['DDC'] == ddc) &\n        (df_historical_forecast['ProductType'] == product) &\n        (df_historical_forecast['dayofweek'] == dow)\n    ].sort_values('date', ascending=False)\n    \n    # Find model function\n    model_func = None\n    for name, func in model_functions:\n        if name == best_model:\n            model_func = func\n            break\n    \n    if model_func is None:\n        forecast = 0\n    else:\n        try:\n            forecast = model_func(route_data, FORECAST_WEEK, FORECAST_YEAR, product)\n            forecast = max(0, forecast)\n        except:\n            forecast = 0\n    \n    # Calculate variance (use ¬±50% as default)\n    variance_pct = 50.0\n    variance_pieces = forecast * (variance_pct / 100)\n    \n    forecast_results.append({\n        'route_key': route['route_key'],\n        'ODC': odc,\n        'DDC': ddc,\n        'ProductType': product,\n        'dayofweek': dow,\n        'week_number': FORECAST_WEEK,\n        'year': FORECAST_YEAR,\n        'forecast': forecast,\n        'optimal_model': best_model,\n        'confidence': route['confidence'],\n        'historical_error_pct': route['best_error'],\n        'forecast_low': max(0, forecast - variance_pieces),\n        'forecast_high': forecast + variance_pieces,\n        'variance_pieces': variance_pieces,\n        'variance_pct': variance_pct\n    })\n\ndf_production_forecast = pd.DataFrame(forecast_results)\nprint(f\"\\n‚úÖ Generated {len(df_production_forecast):,} forecasts for week {FORECAST_WEEK}\")\nprint(f\"   Total forecast: {df_production_forecast['forecast'].sum():,.0f} pieces\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Production Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save production forecast\n",
    "prod_forecast_file = project_root / f'production_forecast_week{FORECAST_WEEK}.csv'\n",
    "df_production_forecast.to_csv(prod_forecast_file, index=False)\n",
    "print(f\"üíæ Saved: {prod_forecast_file.name}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample forecast:\")\n",
    "display(df_production_forecast.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(f\"COMPREHENSIVE WEEKLY UPDATE COMPLETE - {TIMESTAMP}\")\nprint(\"=\"*80)\n\nprint(f\"\\nüìä Evaluation (Week {EVALUATION_WEEK}, {EVALUATION_YEAR}):\")\nprint(f\"  ‚Ä¢ Routes evaluated: {len(df_forecasts):,}\")\nprint(f\"  ‚Ä¢ Actual pieces: {df_actuals['actual_pieces'].sum():,.0f}\")\nprint(f\"  ‚Ä¢ Models tested: {len(model_cols)}\")\n\nprint(f\"\\nüèÜ Top Performing Models:\")\nfor i, (model, count) in enumerate(win_summary.head(5).items(), 1):\n    pct = count / len(df_forecasts) * 100\n    print(f\"  {i}. {model}: {count:,} wins ({pct:.1f}%)\")\n\nprint(f\"\\nüìà Forecast (Week {FORECAST_WEEK}, {FORECAST_YEAR}):\")\nprint(f\"  ‚Ä¢ Routes forecasted: {len(df_production_forecast):,}\")\nprint(f\"  ‚Ä¢ Total forecast: {df_production_forecast['forecast'].sum():,.0f} pieces\")\nprint(f\"  ‚Ä¢ Forecast range: {df_production_forecast['forecast_low'].sum():,.0f} - {df_production_forecast['forecast_high'].sum():,.0f}\")\n\nby_product = df_production_forecast.groupby('ProductType')['forecast'].sum()\nprint(f\"\\n  By Product Type:\")\nfor product, total in by_product.items():\n    print(f\"    ‚Ä¢ {product}: {total:,.0f} pieces\")\n\nprint(f\"\\nüíæ Files Generated:\")\nprint(f\"  ‚Ä¢ {output_file.name}\")\nprint(f\"  ‚Ä¢ {routing_file.name}\")\nprint(f\"  ‚Ä¢ {current_routing_file.name}\")\nprint(f\"  ‚Ä¢ {perf_file.name}\")\nprint(f\"  ‚Ä¢ {prod_forecast_file.name}\")\n\nprint(f\"\\n\" + \"=\"*80)\nprint(\"‚úÖ ALL UPDATES COMPLETE!\")\nprint(\"=\"*80)\n\n# Close connection\nconn.close()\nprint(\"\\nüîå Databricks connection closed\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}